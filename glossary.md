---
layout: page
title: Glossary of Terms
---
{% include JB/setup %}

A glossary of terms we've come across in our travels.

* [Bias and Variance](http://bit.ly/W92qah)  
Given a function *f(x)* which we use to estimate an expected outcome *Y* over a set of inputs *X* in a training set.    
The error rate of that function is the difference between what the function thinks an input should be predicated as and what it actually is in the training set.  
The **bias is the average error** and the **variance describes the spread of those errors**.

* [Cross Entropy](http://en.wikipedia.org/wiki/Cross_entropy)  
This can be used as [an alternative to squared error](http://www.cse.unsw.edu.au/~billw/cs9444/crossentropy.html) when determining the error rate of a learning algorithm. It's particularly useful when we have multiple outputs e.g. with classification problems.

* [log-likelihood](http://en.wikipedia.org/wiki/Likelihood-ratio_test)  
A statistical test used to check the fit of a model for a training set of inputs. 

* [Voronoi Tesselation](http://en.wikipedia.org/wiki/Voronoi_diagram)  
A way of dividing space into a number of regions. In our context it describes the way that a classifier visually divides training data points into their respective classes.